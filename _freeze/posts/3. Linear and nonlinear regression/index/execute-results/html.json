{
  "hash": "b860093c83319c8c07029727ac03d39a",
  "result": {
    "markdown": "---\ntitle: \"Post 3 - Linear and Nonlinear Regression\"\nformat:\n  html:\n    self-contained: true\n    toc: true\n    theme: cosmo\nauthor: \"Charan Teja Chelle\"\ndate: \"2023-10-27\"\ncategories: [code, data visualization]\nimage: \"regression.jpg\"\n---\n\n# Introduction:\n\nRegression is a type of supervised machine learning which is used to capture the relationships between independent and dependent variables, by effectively predicting the continuous output variables (outcome). Few of it's types include linear regression and nonlinear regression. Let's look into the essence of each of these regression types comprehensively.\n\n## Linear Regression\n\nLinear regression assumes a linear relationship between the independent variables and dependent variable, which means that changes in the dependent variable follow those in the independent variables in a linear fashion. Linear regression will be an accurate model only if the observations in the data set are independent of each other. There are many types of linear regression namely simple, multiple, polynomial regression, etc. The primary objective while using any of these linear regression algorithms is to find the best 'Fit Line' equation (refer data visualizations in the below code samples) that can predict the values based on the independent variables.\n\n## Non-Linear Regression\n\nNonlinear regression is a type of polynomial regression, used to model a non-linear relationship between the dependent and independent variables by using nonlinear functions (i.e., including exponentials, logarithms, trigonometric terms). It allows modeling complex relationships allowing better representation of curved patterns in the data. Real-time applications of nonlinear regression include predicting the spread of infectious diseases, modeling population growth over time, etc.,\n\n# Code:\n\n## Code Sample - 1\n\nThis code sample explains non-linear regression with a curvy relationship between the independent variables and the dependent variable. We are using sine function, quadratic term and linear term to generate complex synthetic data with curvy relationship. Moreover, the use of polynomial features lets the model to capture complex relationships beyond linear patterns.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plot\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate synthetic data with a curvy relationship\nnp.random.seed(42)\nX = 3 * np.random.rand(100, 1) - 1.5\ny = 0.5 * X**2 + X + 2 + np.sin(4 * X) + np.random.randn(100, 1)\n\n# Add polynomial features up to degree 10\npoly_features = PolynomialFeatures(degree=10, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\n# Train a linear regression model on the polynomial\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\n\n# Create data points for plotting\nX_new = np.linspace(-1.5, 1.5, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\n\n# Data Visualization using Matplotlib\nplot.figure(figsize=(9, 7))\n\n# Plot the original data points\nplot.scatter(X, y, label='Original Data', color='blue')\n\n# Plot the predicted curve\nplot.plot(X_new, y_new, 'r-', label='Predicted Curve', linewidth=2)\n\n# Add labels and title\nplot.title('Non-Linear Regression with Polynomial Features')\nplot.xlabel('Independent Variable')\nplot.ylabel('Dependent Variable')\n\n# Add legend\nplot.legend()\n\n# Display the results\nplot.show()\n\n# Evaluate the model performance using Mean Squared Error\ny_pred = lin_reg.predict(X_poly)\nmse = mean_squared_error(y, y_pred)\nprint(f'Mean Squared Error: {mse}')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=725 height=597}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.7392821974361152\n```\n:::\n:::\n\n\n## Code Sample - 2\n\nThis relatively simple code sample leverages linear regression generating synthetic data with a linear relationship (4 + 3x + random noise). To visualize the original data points and the regression line, we will be using Matplotlib. The resultant data visualization helps you to understand how well the model captures the underlying linear relationship.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plot\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate synthetic data with a linear relationship\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Train a linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\n\n# Generate data points for plotting the regression line\nX_new = np.linspace(0, 2, 100).reshape(100, 1)\ny_new = lin_reg.predict(X_new)\n\n# Data Visualization using Matplotlib\nplot.figure(figsize=(9, 7))\n\n# Plot the original data points\nplot.scatter(X, y, label='Original Data', color='blue')\n\n# Plot the regression line\nplot.plot(X_new, y_new, 'r-', label='Regression Line', linewidth=2)\n\n# Add labels and title\nplot.title('Linear Regression')\nplot.xlabel('X (Feature)')\nplot.ylabel('y (Target)')\n\n# Add legend\nplot.legend()\n\n# Display the results\nplot.show()\n\n# Evaluate the model performance using Mean Squared Error\ny_pred = lin_reg.predict(X)\nmse = mean_squared_error(y, y_pred)\nprint(f'Mean Squared Error: {mse}')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=733 height=597}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.8065845639670531\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
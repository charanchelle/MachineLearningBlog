{
  "hash": "dfad3f0d43b509f6a41d7f9d86f0f361",
  "result": {
    "markdown": "---\ntitle: \"Post 4 - Classification\"\nformat:\n  html:\n    self-contained: true\n    toc: true\n    theme: cosmo\nauthor: \"Charan Teja Chelle\"\ndate: \"2023-11-19\"\ncategories: [code, data visualization]\nimage: \"classification.jpg\"\n---\n\n# Introduction\n\nClassification is another type of supervised learning in machine learning where the primary objective is to get trained on a labeled dataset in order to predict the class or category of an object or instance. For instance, consider a classification model is trained on a dataset of images labeled as either lions or tigers and then it will predict the class of unseen, new images of lions or tigers solely based on their features such as appearance, color, and shape. Classification is of two types namely Binary (classify input into one of two classes) and Multiclass (classify input into one of several classes) classification. Various types of classification algorithms include Logistic Regression, K-Nearest Neighbors, Naive Bayes, Decision Trees etc., Spam detection and handwriting recognition are some of the real-time applications of classification in machine learning.\n\n# Code\n\nThis code sample illustrates complete pipeline for classification with proper emphasis on data preprocessing, training, testing, and visualization of decision boundaries. In this case, we will be using Iris dataset and K-Nearest Neighbors algorithm for classification and visualizing the results. Furthermore, classification report is also generated along with decision boundaries being plotted effectively.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plot\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Splitting the derived data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features (it's completely optional but often beneficial)\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\n\n# Apply Principal Component Analysis (PCA) for dimensionality reduction to two components\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\n# Train a k-Nearest Neighbors classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=3)\nknn_classifier.fit(X_train_pca, y_train)\n\n# Make predictions on the test set\ny_pred = knn_classifier.predict(X_test_pca)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\n\n# Visualize the decision boundaries\nplot.figure(figsize=(9, 7))\n\n# Plot the training points\nplot.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', edgecolor='k', s=70, label='Training Data')\n\n# Plot the testing points\nplot.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, cmap='viridis', marker='x', s=100, label='Testing Data (True)')\n\n# Plot the decision boundaries\nh = 0.02  # step size in the mesh\nx_min, x_max = X_test_pca[:, 0].min() - 1, X_test_pca[:, 0].max() + 1\ny_min, y_max = X_test_pca[:, 1].min() - 1, X_test_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundaries\nplot.contourf(xx, yy, Z, cmap='viridis', alpha=0.3, levels=np.arange(len(np.unique(y))) - 0.5)\n\n# Add labels and title\nplot.title('k-Nearest Neighbors Classification with Principal Component Analysis')\nplot.xlabel('Principal Component 1')\nplot.ylabel('Principal Component 2')\n\n# Add legend\nplot.legend()\n\n# Display the results\nplot.show()\n\n# Display classification report\nprint('Classification Report:\\n', classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.93\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.78      0.88         9\n           2       0.85      1.00      0.92        11\n\n    accuracy                           0.93        30\n   macro avg       0.95      0.93      0.93        30\nweighted avg       0.94      0.93      0.93        30\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=736 height=597}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
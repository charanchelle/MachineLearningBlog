[
  {
    "objectID": "posts/5. Anomaly (outlier detection)/index.html",
    "href": "posts/5. Anomaly (outlier detection)/index.html",
    "title": "Post 5 - Anomaly/outlier detection",
    "section": "",
    "text": "Introduction\nAnomaly detection, aka outlier detection is a machine learning task that involves identifying patterns or instances in data that do not confine to the expected behavior. These instances are considered anomalies or outliers as they significantly deviate from the majority of data. Anomaly detection can be done in two ways namely Supervised anomaly detection and Unsupervised anomaly detection. Outlier detection plays a vital role in ensuring the security, integrity and reliability of systems by pointing out suspicious or unforeseen events. Few real-time applications of anomaly detection include fraud detection, network security, manufacturing and quality control, healthcare, etc.,\n\n\nCode\nThis code sample demonstates a simple anomaly detection code using ‘Isolation Forest’ algorithm and toy dataset. We will initially be generating a toy dataset with both normal and anomalous data points, then train an Isolation Forest model, and ultimately visualize the results. The data visualizations obtained after running the below code chunk clearly helps us understand how the Isolation Forest algorithm separates normal and anomalous data points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plot\nfrom sklearn.ensemble import IsolationForest\n\n# Create a toy dataset with normal and anomalous data points\nnp.random.seed(42)\nnormal_data = np.random.normal(loc=0, scale=1, size=(800, 2))  # Normal data points\nanomalous_data = np.random.normal(loc=5, scale=1, size=(20, 2))  # Anomalous data points\n\n# Merge normal and anomalous data\ndata = np.vstack([normal_data, anomalous_data])\n\n# Shuffle the data\nnp.random.shuffle(data)\n\n# Visualize the dataset\nplot.figure(figsize=(9, 7))\nplot.scatter(data[:, 0], data[:, 1], c='b', marker='o', label='Normal Data', alpha=0.7)\nplot.scatter(anomalous_data[:, 0], anomalous_data[:, 1], c='r', marker='x', label='Anomalous Data')\nplot.title('Toy Dataset for Anomaly Detection')\nplot.xlabel('Feature 1')\nplot.ylabel('Feature 2')\nplot.legend()\nplot.show()\n\n# Train an Isolation Forest model by setting 'contamination' parameter to expected proportion of anomalies in dataset\nisolation_forest = IsolationForest(contamination=0.02, random_state=42)\nisolation_forest.fit(data)\n\n# Predict anomalies (1 for normal, -1 for anomaly)\npredictions = isolation_forest.predict(data)\n\n# Visualize the Isolation Forest results\nplot.figure(figsize=(9, 7))\n\n# Plot normal data points\nplot.scatter(data[predictions == 1][:, 0], data[predictions == 1][:, 1], c='b', marker='o', label='Normal Data')\n\n# Plot anomalous data points\nplot.scatter(data[predictions == -1][:, 0], data[predictions == -1][:, 1], c='r', marker='x', label='Anomalous Data')\n\n# Highlight true anomalous data points\nplot.scatter(anomalous_data[:, 0], anomalous_data[:, 1], c='g', marker='*', s=200, label='True Anomalies')\n\nplot.title('Isolation Forest Anomaly Detection')\nplot.xlabel('Feature 1')\nplot.ylabel('Feature 2')\nplot.legend()\nplot.show()"
  },
  {
    "objectID": "posts/3. Linear and nonlinear regression/index.html",
    "href": "posts/3. Linear and nonlinear regression/index.html",
    "title": "Post 3 - Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression is a type of supervised machine learning which is used to capture the relationships between independent and dependent variables, by effectively predicting the continuous output variables (outcome). Few of it’s types include linear regression and nonlinear regression. Let’s look into the essence of each of these regression types comprehensively.\n\n\nLinear regression assumes a linear relationship between the independent variables and dependent variable, which means that changes in the dependent variable follow those in the independent variables in a linear fashion. Linear regression will be an accurate model only if the observations in the data set are independent of each other. There are many types of linear regression namely simple, multiple, polynomial regression, etc. The primary objective while using any of these linear regression algorithms is to find the best ‘Fit Line’ equation (refer data visualizations in the below code samples) that can predict the values based on the independent variables.\n\n\n\nNonlinear regression is a type of polynomial regression, used to model a non-linear relationship between the dependent and independent variables by using nonlinear functions (i.e., including exponentials, logarithms, trigonometric terms). It allows modeling complex relationships allowing better representation of curved patterns in the data. Real-time applications of nonlinear regression include predicting the spread of infectious diseases, modeling population growth over time, etc.,"
  },
  {
    "objectID": "posts/3. Linear and nonlinear regression/index.html#linear-regression",
    "href": "posts/3. Linear and nonlinear regression/index.html#linear-regression",
    "title": "Post 3 - Linear and Nonlinear Regression",
    "section": "",
    "text": "Linear regression assumes a linear relationship between the independent variables and dependent variable, which means that changes in the dependent variable follow those in the independent variables in a linear fashion. Linear regression will be an accurate model only if the observations in the data set are independent of each other. There are many types of linear regression namely simple, multiple, polynomial regression, etc. The primary objective while using any of these linear regression algorithms is to find the best ‘Fit Line’ equation (refer data visualizations in the below code samples) that can predict the values based on the independent variables."
  },
  {
    "objectID": "posts/3. Linear and nonlinear regression/index.html#non-linear-regression",
    "href": "posts/3. Linear and nonlinear regression/index.html#non-linear-regression",
    "title": "Post 3 - Linear and Nonlinear Regression",
    "section": "",
    "text": "Nonlinear regression is a type of polynomial regression, used to model a non-linear relationship between the dependent and independent variables by using nonlinear functions (i.e., including exponentials, logarithms, trigonometric terms). It allows modeling complex relationships allowing better representation of curved patterns in the data. Real-time applications of nonlinear regression include predicting the spread of infectious diseases, modeling population growth over time, etc.,"
  },
  {
    "objectID": "posts/3. Linear and nonlinear regression/index.html#code-sample---1",
    "href": "posts/3. Linear and nonlinear regression/index.html#code-sample---1",
    "title": "Post 3 - Linear and Nonlinear Regression",
    "section": "Code Sample - 1",
    "text": "Code Sample - 1\nThis code sample explains non-linear regression with a curvy relationship between the independent variables and the dependent variable. We are using sine function, quadratic term and linear term to generate complex synthetic data with curvy relationship. Moreover, the use of polynomial features lets the model to capture complex relationships beyond linear patterns.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plot\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate synthetic data with a curvy relationship\nnp.random.seed(42)\nX = 3 * np.random.rand(100, 1) - 1.5\ny = 0.5 * X**2 + X + 2 + np.sin(4 * X) + np.random.randn(100, 1)\n\n# Add polynomial features up to degree 10\npoly_features = PolynomialFeatures(degree=10, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\n# Train a linear regression model on the polynomial\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\n\n# Create data points for plotting\nX_new = np.linspace(-1.5, 1.5, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\n\n# Data Visualization using Matplotlib\nplot.figure(figsize=(9, 7))\n\n# Plot the original data points\nplot.scatter(X, y, label='Original Data', color='blue')\n\n# Plot the predicted curve\nplot.plot(X_new, y_new, 'r-', label='Predicted Curve', linewidth=2)\n\n# Add labels and title\nplot.title('Non-Linear Regression with Polynomial Features')\nplot.xlabel('Independent Variable')\nplot.ylabel('Dependent Variable')\n\n# Add legend\nplot.legend()\n\n# Display the results\nplot.show()\n\n# Evaluate the model performance using Mean Squared Error\ny_pred = lin_reg.predict(X_poly)\nmse = mean_squared_error(y, y_pred)\nprint(f'Mean Squared Error: {mse}')\n\n\n\n\n\nMean Squared Error: 0.7392821974361152"
  },
  {
    "objectID": "posts/3. Linear and nonlinear regression/index.html#code-sample---2",
    "href": "posts/3. Linear and nonlinear regression/index.html#code-sample---2",
    "title": "Post 3 - Linear and Nonlinear Regression",
    "section": "Code Sample - 2",
    "text": "Code Sample - 2\nThis relatively simple code sample leverages linear regression generating synthetic data with a linear relationship (4 + 3x + random noise). To visualize the original data points and the regression line, we will be using Matplotlib. The resultant data visualization helps you to understand how well the model captures the underlying linear relationship.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plot\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate synthetic data with a linear relationship\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Train a linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\n\n# Generate data points for plotting the regression line\nX_new = np.linspace(0, 2, 100).reshape(100, 1)\ny_new = lin_reg.predict(X_new)\n\n# Data Visualization using Matplotlib\nplot.figure(figsize=(9, 7))\n\n# Plot the original data points\nplot.scatter(X, y, label='Original Data', color='blue')\n\n# Plot the regression line\nplot.plot(X_new, y_new, 'r-', label='Regression Line', linewidth=2)\n\n# Add labels and title\nplot.title('Linear Regression')\nplot.xlabel('X (Feature)')\nplot.ylabel('y (Target)')\n\n# Add legend\nplot.legend()\n\n# Display the results\nplot.show()\n\n# Evaluate the model performance using Mean Squared Error\ny_pred = lin_reg.predict(X)\nmse = mean_squared_error(y, y_pred)\nprint(f'Mean Squared Error: {mse}')\n\n\n\n\n\nMean Squared Error: 0.8065845639670531"
  },
  {
    "objectID": "posts/1. Probability theory and random variables/index.html",
    "href": "posts/1. Probability theory and random variables/index.html",
    "title": "Post 1 - Probability Theory and Random Variables",
    "section": "",
    "text": "Two fundamental ideas in the wide field of machine learning - Random Variables and Probability Theory, pave way for reliable algorithms and accurate prediction, mitigating uncertainty to maximum possible extent. In this blog post, we will delve deeper into the core principles of these concepts and take a loot at their significance and relevance in current industry.\n\n\nProbability theory provides a consistent framework for the quantification and manipulation of uncertainty with discrete and continuous probability distributions. It also extends its arms into Bayesian inference and Conditional probability, which are the backbone of probabilistic modeling.\n\n\n\nRandom variables (discrete and continuous) assume the numerical value of all possible outcomes of an experiment, serving as a bridge between probability theory and data."
  },
  {
    "objectID": "posts/1. Probability theory and random variables/index.html#probability-theory",
    "href": "posts/1. Probability theory and random variables/index.html#probability-theory",
    "title": "Post 1 - Probability Theory and Random Variables",
    "section": "",
    "text": "Probability theory provides a consistent framework for the quantification and manipulation of uncertainty with discrete and continuous probability distributions. It also extends its arms into Bayesian inference and Conditional probability, which are the backbone of probabilistic modeling."
  },
  {
    "objectID": "posts/1. Probability theory and random variables/index.html#random-variables",
    "href": "posts/1. Probability theory and random variables/index.html#random-variables",
    "title": "Post 1 - Probability Theory and Random Variables",
    "section": "",
    "text": "Random variables (discrete and continuous) assume the numerical value of all possible outcomes of an experiment, serving as a bridge between probability theory and data."
  },
  {
    "objectID": "posts/1. Probability theory and random variables/index.html#code-sample---1",
    "href": "posts/1. Probability theory and random variables/index.html#code-sample---1",
    "title": "Post 1 - Probability Theory and Random Variables",
    "section": "Code Sample - 1",
    "text": "Code Sample - 1\nThis is a code sample that involves generating random variables and computing probabilities. A six-sided die is rolled 20 times (input) and probabilities of each outcome is calculated and displayed accordingly (output)\n\n\nCode\nimport random\n\n# Generating a Random Variable - Rolling a six-sided die\ndef roll_die():\n    return random.randint(1, 6)\n\n# Simulating an Experiment - Rolling a six-sided die 20 times\nnumber_of_rolls = 20\nall_rolls = [roll_die() for _ in range(number_of_rolls)]\n\n# Computing Probability Distribution\ndef compute_probability(dice_outcomes):\n    probabilities = {i: dice_outcomes.count(i) / len(dice_outcomes) for i in set(dice_outcomes)}\n    return probabilities\n\n# Displaying Results\nprint(f\"Simulated Rolls: {all_rolls}\")\nprint(\"\\nProbability Distribution:\")\nfor outcome, probability in compute_probability(all_rolls).items():\n    print(f\"Outcome {outcome}: Probability {probability:.2f}\")\n\n\nSimulated Rolls: [2, 4, 1, 6, 1, 5, 3, 3, 2, 6, 1, 4, 4, 1, 2, 4, 5, 1, 5, 1]\n\nProbability Distribution:\nOutcome 1: Probability 0.30\nOutcome 2: Probability 0.15\nOutcome 3: Probability 0.10\nOutcome 4: Probability 0.20\nOutcome 5: Probability 0.15\nOutcome 6: Probability 0.10"
  },
  {
    "objectID": "posts/1. Probability theory and random variables/index.html#code-sample---2",
    "href": "posts/1. Probability theory and random variables/index.html#code-sample---2",
    "title": "Post 1 - Probability Theory and Random Variables",
    "section": "Code Sample - 2",
    "text": "Code Sample - 2\nThis relatively complex code sample deals with conditional probability where ‘probability of playing cricket given the weather’ is already declared; and weather conditions for 20 days are simulated, which serves as input; output will be finding cricket playing possibilities for 20 days, backed with the data visualization table stating ‘weather conditions’ on the X-axis and ‘cricket playing possibilities’ on Y-axis with scattered points plotting conditional probability.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Simulating Data - Weather and Cricket Playing\nnp.random.seed(45)\n\n# Input: Simulating weather conditions (0: Sunny, 1: Overcast, 2: Rainy) for 20 days\nweather_conditions = np.random.choice([0, 1, 2], size=20, p=[0.5, 0.2, 0.3])\n\n# Conditional Probability - Probability of playing cricket given the weather\nprob_play_cricket_given_sunny = 0.9\nprob_play_cricket_given_overcast = 0.95\nprob_play_cricket_given_rainy = 0.6\n\nplay_cricket = []\n\nfor weather in weather_conditions:\n    if weather == 0:\n        play_cricket.append(np.random.choice([1, 0], p=[prob_play_cricket_given_sunny, 1 - prob_play_cricket_given_sunny]))\n    elif weather == 1:\n        play_cricket.append(np.random.choice([1, 0], p=[prob_play_cricket_given_overcast, 1 - prob_play_cricket_given_overcast]))\n    else:\n        play_cricket.append(np.random.choice([1, 0], p=[prob_play_cricket_given_rainy, 1 - prob_play_cricket_given_rainy]))\n\n# Displaying Results\nprint(f\"Simulated Weather probabilities for 20 days: {weather_conditions}\")\nprint(f\"\\nResultant Cricket playing probabilities for 20 days: {play_cricket}\")\n\n# Data Visualization using Seaborn and Matplotlib\nplt.figure(figsize=(9, 7))\n\n# Plotting Conditional Probability with Swarm Plot\nsns.swarmplot(x=weather_conditions, y=play_cricket,hue=weather_conditions, palette=\"pastel\", size=8)\nplt.title('Conditional Probability: Cricket Playing Given Weather')\nplt.xlabel('Weather Conditions (0: Sunny, 1: Overcast, 2: Rainy)')\nplt.ylabel('Play Cricket (1: Yes, 0: No)')\n\n# Displaying Results\nplt.show()\n\n\nSimulated Weather probabilities for 20 days: [2 1 0 0 0 0 0 0 0 1 2 1 2 0 1 0 2 1 0 0]\n\nResultant Cricket playing probabilities for 20 days: [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog predominantly focuses on various posts pertaining to topics that play critical roles in different aspects of machine learning. Topics include Probability theory and random variables, Clustering, Linear and nonlinear regression, Classification, Anomaly (outlier detection). The significance and relevance of all these topics are explained comprehensively (along with code samples and data visualization) in the blog posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Machine Learning (CS5805) Blog",
    "section": "",
    "text": "Post 5 - Anomaly/outlier detection\n\n\n\n\n\n\n\ncode\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nCharan Teja Chelle\n\n\n\n\n\n\n  \n\n\n\n\nPost 4 - Classification\n\n\n\n\n\n\n\ncode\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nCharan Teja Chelle\n\n\n\n\n\n\n  \n\n\n\n\nPost 3 - Linear and Nonlinear Regression\n\n\n\n\n\n\n\ncode\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nCharan Teja Chelle\n\n\n\n\n\n\n  \n\n\n\n\nPost 2 - Clustering\n\n\n\n\n\n\n\ncode\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nCharan Teja Chelle\n\n\n\n\n\n\n  \n\n\n\n\nPost 1 - Probability Theory and Random Variables\n\n\n\n\n\n\n\ncode\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nCharan Teja Chelle\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2. Clustering/index.html",
    "href": "posts/2. Clustering/index.html",
    "title": "Post 2 - Clustering",
    "section": "",
    "text": "Introduction\nClustering is more of an unsupervised machine learning where unlabeled similar examples are grouped into clusters in order to understand a data set in a system. The fundamental difference between clustering and classification is that if these examples are labeled, then clustering becomes classification. Grouping is done with the help of a metric called ‘similarity measure’, which can be obtained with the help of examples’ feature data. So, the primary objective of clustering is to discover natural groupings or structures in the data that might not be explicitly defined. There are various types of clustering namely hierarchical, agglomerative, k-means. Few of clustering applications in machine learning include market segmentation, image segmentation, search result grouping, document clustering, anomaly detection and social network analysis.\n\n\nCode\nIn this code sample, we are visualizing the results using synthetic data applying K-Means clustering algorithm (where Scikit-learn library is used for the machine learning part; Matplotlib and Seaborn for data visualization). The resultant plot includes original data points, cluster data points and cluster centers with effective clarity of data visualization.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plot\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n\n# Create synthetic data with three clusters\nX, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n\n# Explicitly set n_init to suppress warning\nkmeans_cluster = KMeans(n_clusters=3, random_state=42, n_init=10)  # You can dynamically set n_init to another value\ny_kmeans_cluster = kmeans_cluster.fit_predict(X)\n\n# Data Visualization using Seaborn and Matplotlib\nplot.figure(figsize=(9, 7))\n\n# Plot the original data points\nplot.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=70, label='Original Data')\n\n# Plot the cluster centers\nplot.scatter(kmeans_cluster.cluster_centers_[:, 0], kmeans_cluster.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n\n# Plot the clustered data points\nplot.scatter(X[:, 0], X[:, 1], c=y_kmeans_cluster, cmap='viridis', edgecolor='k', s=50, label='Clustered Data')\n\n# Add labels and title\nplot.title('K-Means Clustering')\nplot.xlabel('Feature 1')\nplot.ylabel('Feature 2')\n\n# Add legend\nplot.legend()\n\n# Display the results\nplot.show()"
  },
  {
    "objectID": "posts/4. Classification/index.html",
    "href": "posts/4. Classification/index.html",
    "title": "Post 4 - Classification",
    "section": "",
    "text": "Introduction\nClassification is another type of supervised learning in machine learning where the primary objective is to get trained on a labeled dataset in order to predict the class or category of an object or instance. For instance, consider a classification model is trained on a dataset of images labeled as either lions or tigers and then it will predict the class of unseen, new images of lions or tigers solely based on their features such as appearance, color, and shape. Classification is of two types namely Binary (classify input into one of two classes) and Multiclass (classify input into one of several classes) classification. Various types of classification algorithms include Logistic Regression, K-Nearest Neighbors, Naive Bayes, Decision Trees etc., Spam detection and handwriting recognition are some of the real-time applications of classification in machine learning.\n\n\nCode\nThis code sample illustrates complete pipeline for classification with proper emphasis on data preprocessing, training, testing, and visualization of decision boundaries. In this case, we will be using Iris dataset and K-Nearest Neighbors algorithm for classification and visualizing the results. Furthermore, classification report is also generated along with decision boundaries being plotted effectively.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plot\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Splitting the derived data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features (it's completely optional but often beneficial)\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\n\n# Apply Principal Component Analysis (PCA) for dimensionality reduction to two components\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\n# Train a k-Nearest Neighbors classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=3)\nknn_classifier.fit(X_train_pca, y_train)\n\n# Make predictions on the test set\ny_pred = knn_classifier.predict(X_test_pca)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\n\n# Visualize the decision boundaries\nplot.figure(figsize=(9, 7))\n\n# Plot the training points\nplot.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', edgecolor='k', s=70, label='Training Data')\n\n# Plot the testing points\nplot.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, cmap='viridis', marker='x', s=100, label='Testing Data (True)')\n\n# Plot the decision boundaries\nh = 0.02  # step size in the mesh\nx_min, x_max = X_test_pca[:, 0].min() - 1, X_test_pca[:, 0].max() + 1\ny_min, y_max = X_test_pca[:, 1].min() - 1, X_test_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundaries\nplot.contourf(xx, yy, Z, cmap='viridis', alpha=0.3, levels=np.arange(len(np.unique(y))) - 0.5)\n\n# Add labels and title\nplot.title('k-Nearest Neighbors Classification with Principal Component Analysis')\nplot.xlabel('Principal Component 1')\nplot.ylabel('Principal Component 2')\n\n# Add legend\nplot.legend()\n\n# Display the results\nplot.show()\n\n# Display classification report\nprint('Classification Report:\\n', classification_report(y_test, y_pred))\n\n\nAccuracy: 0.93\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.78      0.88         9\n           2       0.85      1.00      0.92        11\n\n    accuracy                           0.93        30\n   macro avg       0.95      0.93      0.93        30\nweighted avg       0.94      0.93      0.93        30"
  }
]